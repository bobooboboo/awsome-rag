#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ° Embedding æ¨¡å‹æµ‹è¯•ç¤ºä¾‹

è¯¥ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ï¼š
1. åˆå§‹åŒ–æœ¬åœ° embedding æ¨¡å‹
2. æµ‹è¯•æ–‡æœ¬åµŒå…¥åŠŸèƒ½
3. æµ‹è¯•æŸ¥è¯¢åµŒå…¥åŠŸèƒ½
4. éªŒè¯åµŒå…¥å‘é‡çš„ç»´åº¦å’Œè´¨é‡
5. æµ‹è¯•æ‰¹é‡åµŒå…¥
6. è®¡ç®—ç›¸ä¼¼åº¦
7. æ€§èƒ½åŸºå‡†æµ‹è¯•

ä½¿ç”¨å‰è¯·ç¡®ä¿ï¼š
- å·²å®‰è£… ollama
- å·²ä¸‹è½½æŒ‡å®šçš„æ¨¡å‹ï¼ˆdengcao/Qwen3-Embedding-8B:Q5_K_Mï¼‰
- ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼ˆé»˜è®¤ç«¯å£ 11434ï¼‰

è¿è¡Œå‘½ä»¤ï¼š
python examples/test_local_embedding.py
"""

import os
import sys
import time
import numpy as np
from typing import List, Dict, Any
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.model.embedding_model import LocalEmbeddingModel, get_embedding_model
from app.config.config import LOCAL_EMBED_MODEL_CONFIG, EMBED_MODEL_TYPE


def test_basic_embedding():
    """æµ‹è¯•åŸºæœ¬çš„åµŒå…¥åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ”¬ æµ‹è¯•åŸºæœ¬åµŒå…¥åŠŸèƒ½")
    print("=" * 60)
    
    # åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹
    try:
        embedding_model = LocalEmbeddingModel()
        print(f"âœ… æˆåŠŸåˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹: {LOCAL_EMBED_MODEL_CONFIG['model_name']}")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–æ¨¡å‹å¤±è´¥: {e}")
        return None
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œé˜³å…‰æ˜åªšã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å‘å±•è¿…é€Ÿã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­åº”ç”¨å¹¿æ³›ã€‚"
    ]
    
    print("\nğŸ“ æµ‹è¯•æ–‡æœ¬åˆ—è¡¨:")
    for i, text in enumerate(test_texts, 1):
        print(f"  {i}. {text}")
    
    # æµ‹è¯•å•ä¸ªæ–‡æœ¬åµŒå…¥
    print("\nğŸ” æµ‹è¯•å•ä¸ªæ–‡æœ¬åµŒå…¥:")
    try:
        text = test_texts[0]
        start_time = time.time()
        embedding = embedding_model._get_text_embedding(text)
        end_time = time.time()
        
        print(f"  è¾“å…¥æ–‡æœ¬: {text}")
        print(f"  åµŒå…¥ç»´åº¦: {len(embedding)}")
        print(f"  å‘é‡ç±»å‹: {type(embedding)}")
        print(f"  å‰5ä¸ªå€¼: {embedding[:5]}")
        print(f"  è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
        # éªŒè¯å‘é‡æ˜¯å¦æœ‰æ•ˆ
        if embedding and len(embedding) > 0:
            print("  âœ… æ–‡æœ¬åµŒå…¥æˆåŠŸ")
        else:
            print("  âŒ æ–‡æœ¬åµŒå…¥å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"  âŒ æ–‡æœ¬åµŒå…¥å¼‚å¸¸: {e}")
        return None
    
    return embedding_model


def test_query_embedding(embedding_model: LocalEmbeddingModel):
    """æµ‹è¯•æŸ¥è¯¢åµŒå…¥åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯•æŸ¥è¯¢åµŒå…¥åŠŸèƒ½")
    print("=" * 60)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "ä»Šå¤©çš„å¤©æ°”å¦‚ä½•ï¼Ÿ",
        "NLPæŠ€æœ¯æœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
    ]
    
    print("\nğŸ“‹ æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨:")
    for i, query in enumerate(test_queries, 1):
        print(f"  {i}. {query}")
    
    try:
        query = test_queries[0]
        start_time = time.time()
        query_embedding = embedding_model._get_query_embedding(query)
        end_time = time.time()
        
        print(f"\nğŸ” æŸ¥è¯¢åµŒå…¥ç»“æœ:")
        print(f"  æŸ¥è¯¢æ–‡æœ¬: {query}")
        print(f"  åµŒå…¥ç»´åº¦: {len(query_embedding)}")
        print(f"  å‰5ä¸ªå€¼: {query_embedding[:5]}")
        print(f"  è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
        if query_embedding and len(query_embedding) > 0:
            print("  âœ… æŸ¥è¯¢åµŒå…¥æˆåŠŸ")
        else:
            print("  âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥")
            
    except Exception as e:
        print(f"  âŒ æŸ¥è¯¢åµŒå…¥å¼‚å¸¸: {e}")


def test_batch_embedding(embedding_model: LocalEmbeddingModel):
    """æµ‹è¯•æ‰¹é‡åµŒå…¥"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ æµ‹è¯•æ‰¹é‡åµŒå…¥")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡æœ¬æ‰¹æ¬¡
    batch_texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
        "æ·±åº¦å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        "è‡ªç„¶è¯­è¨€å¤„ç†è®©æœºå™¨ç†è§£äººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰è®©æœºå™¨çœ‹æ‡‚ä¸–ç•Œ",
        "æ¨èç³»ç»Ÿæå‡ç”¨æˆ·ä½“éªŒ",
        "çŸ¥è¯†å›¾è°±æ„å»ºæ™ºèƒ½ç³»ç»Ÿ",
        "å¼ºåŒ–å­¦ä¹ å®ç°æ™ºèƒ½å†³ç­–",
        "ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘å·¥ä½œ"
    ]
    
    print(f"\nğŸ“‹ æ‰¹é‡å¤„ç† {len(batch_texts)} ä¸ªæ–‡æœ¬:")
    for i, text in enumerate(batch_texts, 1):
        print(f"  {i}. {text}")
    
    try:
        start_time = time.time()
        embeddings = []
        
        # é€ä¸ªå¤„ç†ï¼ˆæ¨¡æ‹Ÿæ‰¹é‡ï¼‰
        for text in batch_texts:
            embedding = embedding_model._get_text_embedding(text)
            embeddings.append(embedding)
        
        end_time = time.time()
        
        print(f"\nğŸ“Š æ‰¹é‡åµŒå…¥ç»“æœ:")
        print(f"  å¤„ç†æ–‡æœ¬æ•°: {len(embeddings)}")
        print(f"  æ€»è€—æ—¶: {end_time - start_time:.3f}ç§’")
        print(f"  å¹³å‡æ¯ä¸ª: {(end_time - start_time) / len(batch_texts):.3f}ç§’")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0]) if embeddings else 0}")
        print("  âœ… æ‰¹é‡åµŒå…¥æˆåŠŸ")
        
        return embeddings
        
    except Exception as e:
        print(f"  âŒ æ‰¹é‡åµŒå…¥å¼‚å¸¸: {e}")
        return []


def test_similarity_calculation(embeddings: List[List[float]]):
    """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\n" + "=" * 60)
    print("ğŸ¯ æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—")
    print("=" * 60)
    
    if len(embeddings) < 2:
        print("âŒ éœ€è¦è‡³å°‘2ä¸ªåµŒå…¥å‘é‡æ¥è®¡ç®—ç›¸ä¼¼åº¦")
        return
    
    try:
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings_array = np.array(embeddings)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(embeddings_array)
        
        print(f"\nğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ ({len(embeddings)}x{len(embeddings)}):")
        
        # æ˜¾ç¤ºç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
        print("     ", end="")
        for i in range(len(embeddings)):
            print(f"  {i:2d}  ", end="")
        print()
        
        for i in range(len(embeddings)):
            print(f"  {i:2d} ", end="")
            for j in range(len(embeddings)):
                print(f"{similarity_matrix[i][j]:5.3f}", end=" ")
            print()
        
        # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„æ–‡æœ¬å¯¹ï¼ˆæ’é™¤è‡ªèº«ï¼‰
        max_similarity = 0
        max_pair = (0, 1)
        
        for i in range(len(embeddings)):
            for j in range(i + 1, len(embeddings)):
                if similarity_matrix[i][j] > max_similarity:
                    max_similarity = similarity_matrix[i][j]
                    max_pair = (i, j)
        
        print(f"\nğŸ† æœ€ç›¸ä¼¼çš„æ–‡æœ¬å¯¹:")
        print(f"  æ–‡æœ¬ {max_pair[0]} å’Œæ–‡æœ¬ {max_pair[1]}")
        print(f"  ç›¸ä¼¼åº¦: {max_similarity:.4f}")
        print("  âœ… ç›¸ä¼¼åº¦è®¡ç®—æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¼‚å¸¸: {e}")


def test_vector_quality(embedding_model: LocalEmbeddingModel):
    """æµ‹è¯•å‘é‡è´¨é‡"""
    print("\n" + "=" * 60)
    print("â­ æµ‹è¯•å‘é‡è´¨é‡")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æ–‡æœ¬
    test_cases = [
        {
            "category": "ç›¸ä¼¼æ–‡æœ¬",
            "texts": [
                "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
                "äººå·¥æ™ºèƒ½åŒ…å«æœºå™¨å­¦ä¹ æŠ€æœ¯"
            ]
        },
        {
            "category": "ä¸åŒä¸»é¢˜",
            "texts": [
                "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªš",
                "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œè®­ç»ƒ"
            ]
        },
        {
            "category": "è¯­è¨€å·®å¼‚",
            "texts": [
                "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
                "Natural Language Processing"
            ]
        }
    ]
    
    for case in test_cases:
        print(f"\nğŸ”¬ æµ‹è¯•æ¡ˆä¾‹: {case['category']}")
        try:
            embeddings = []
            for text in case['texts']:
                embedding = embedding_model._get_text_embedding(text)
                embeddings.append(embedding)
                print(f"  æ–‡æœ¬: {text}")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            if len(embeddings) == 2:
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                print(f"  ç›¸ä¼¼åº¦: {similarity:.4f}")
                
                # è´¨é‡è¯„ä¼°
                if case['category'] == "ç›¸ä¼¼æ–‡æœ¬" and similarity > 0.7:
                    print("  âœ… ç›¸ä¼¼æ–‡æœ¬æœ‰é«˜ç›¸ä¼¼åº¦ï¼Œå‘é‡è´¨é‡è‰¯å¥½")
                elif case['category'] == "ä¸åŒä¸»é¢˜" and similarity < 0.5:
                    print("  âœ… ä¸åŒä¸»é¢˜æœ‰ä½ç›¸ä¼¼åº¦ï¼Œå‘é‡åŒºåˆ†æ€§è‰¯å¥½")
                elif case['category'] == "è¯­è¨€å·®å¼‚":
                    print(f"  ğŸ“ è·¨è¯­è¨€ç›¸ä¼¼åº¦: {similarity:.4f}")
                else:
                    print("  âš ï¸  å‘é‡è´¨é‡éœ€è¦è¿›ä¸€æ­¥éªŒè¯")
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•æ¡ˆä¾‹å¼‚å¸¸: {e}")


def test_performance_benchmark(embedding_model: LocalEmbeddingModel):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # ä¸åŒé•¿åº¦çš„æ–‡æœ¬
    test_texts = {
        "çŸ­æ–‡æœ¬": "AIæŠ€æœ¯",
        "ä¸­ç­‰æ–‡æœ¬": "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯",
        "é•¿æ–‡æœ¬": "éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ç­‰AIåº”ç”¨é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚è¿™äº›æŠ€æœ¯ä¸ä»…åœ¨å­¦æœ¯ç ”ç©¶ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¹Ÿåœ¨å·¥ä¸šç•Œå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥äº†å·¨å¤§çš„å˜é©å’Œä¾¿åˆ©ã€‚"
    }
    
    for text_type, text in test_texts.items():
        print(f"\nğŸ“ æµ‹è¯• {text_type} (é•¿åº¦: {len(text)} å­—ç¬¦):")
        print(f"  æ–‡æœ¬: {text[:50]}...")
        
        try:
            # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
            times = []
            for _ in range(3):
                start_time = time.time()
                embedding = embedding_model._get_text_embedding(text)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            print(f"  å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
            print(f"  æœ€å¿«è€—æ—¶: {min_time:.3f}ç§’")
            print(f"  æœ€æ…¢è€—æ—¶: {max_time:.3f}ç§’")
            print(f"  å‘é‡ç»´åº¦: {len(embedding)}")
            
            # è®¡ç®—ååç‡
            throughput = len(text) / avg_time
            print(f"  å¤„ç†é€Ÿåº¦: {throughput:.1f} å­—ç¬¦/ç§’")
            
        except Exception as e:
            print(f"  âŒ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")


def test_model_consistency(embedding_model: LocalEmbeddingModel):
    """æµ‹è¯•æ¨¡å‹ä¸€è‡´æ€§"""
    print("\n" + "=" * 60)
    print("ğŸ”„ æµ‹è¯•æ¨¡å‹ä¸€è‡´æ€§")
    print("=" * 60)
    
    test_text = "æµ‹è¯•æ¨¡å‹è¾“å‡ºä¸€è‡´æ€§"
    print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
    
    try:
        embeddings = []
        print("\nè¿›è¡Œ5æ¬¡åµŒå…¥æµ‹è¯•:")
        
        for i in range(5):
            embedding = embedding_model._get_text_embedding(test_text)
            embeddings.append(embedding)
            print(f"  ç¬¬{i+1}æ¬¡: å‰3ä¸ªå€¼ {embedding[:3]}")
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        if all(np.array_equal(embeddings[0], emb) for emb in embeddings[1:]):
            print("  âœ… æ¨¡å‹è¾“å‡ºå®Œå…¨ä¸€è‡´")
        else:
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for i in range(1, len(embeddings)):
                sim = cosine_similarity([embeddings[0]], [embeddings[i]])[0][0]
                similarities.append(sim)
            
            avg_similarity = sum(similarities) / len(similarities)
            print(f"  ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.6f}")
            
            if avg_similarity > 0.999:
                print("  âœ… æ¨¡å‹è¾“å‡ºé«˜åº¦ä¸€è‡´")
            else:
                print("  âš ï¸  æ¨¡å‹è¾“å‡ºå­˜åœ¨å·®å¼‚")
                
    except Exception as e:
        print(f"âŒ ä¸€è‡´æ€§æµ‹è¯•å¼‚å¸¸: {e}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æœ¬åœ° Embedding æ¨¡å‹æµ‹è¯•å¼€å§‹")
    print(f"ğŸ“‹ å½“å‰é…ç½®:")
    print(f"  æ¨¡å‹ç±»å‹: {EMBED_MODEL_TYPE}")
    print(f"  æ¨¡å‹åç§°: {LOCAL_EMBED_MODEL_CONFIG['model_name']}")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if EMBED_MODEL_TYPE != "local":
        print("âš ï¸  å½“å‰é…ç½®ä¸æ˜¯æœ¬åœ°æ¨¡å‹ï¼Œè¯·è®¾ç½® EMBED_MODEL_TYPE=local")
        return
    
    # 1. åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    embedding_model = test_basic_embedding()
    if not embedding_model:
        print("âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # 2. æŸ¥è¯¢åµŒå…¥æµ‹è¯•
    test_query_embedding(embedding_model)
    
    # 3. æ‰¹é‡åµŒå…¥æµ‹è¯•
    embeddings = test_batch_embedding(embedding_model)
    
    # 4. ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•
    if embeddings:
        test_similarity_calculation(embeddings)
    
    # 5. å‘é‡è´¨é‡æµ‹è¯•
    test_vector_quality(embedding_model)
    
    # 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
    test_performance_benchmark(embedding_model)
    
    # 7. æ¨¡å‹ä¸€è‡´æ€§æµ‹è¯•
    test_model_consistency(embedding_model)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    
    # æ€»ç»“å»ºè®®
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("  1. ç¡®ä¿ ollama æœåŠ¡æ­£å¸¸è¿è¡Œ")
    print("  2. æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹")
    print("  3. ç›‘æ§åµŒå…¥è´¨é‡å’Œæ€§èƒ½æŒ‡æ ‡")
    print("  4. å®šæœŸæµ‹è¯•æ¨¡å‹ä¸€è‡´æ€§")


if __name__ == "__main__":
    main() 