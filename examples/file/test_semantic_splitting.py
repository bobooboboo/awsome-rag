from llama_index.core import Document
from llama_index.core.node_parser import SemanticSplitterNodeParser, SentenceSplitter

from app.data_indexing.file.document_loader.local_file import LocalFileLoader
from app.data_indexing.file.document_splitter.document_splitter_factory import DocumentSplitterFactory

if __name__ == '__main__':
    # åŠ è½½é•¿æ–‡æœ¬æµ‹è¯•æ–‡ä»¶
    file_path = "/Users/boboo/awsome_rag/awsome-rag/test_semantic_long.txt"
    documents = LocalFileLoader().load_documents(file_path)
    
    print("=" * 60)
    print("è¯­ä¹‰æ‹†åˆ†æ•ˆæœæµ‹è¯• - é•¿æ–‡æœ¬")
    print("=" * 60)
    
    # æ–‡æ¡£ä¿¡æ¯
    print(f"\nğŸ“„ åŸæ–‡æ¡£ä¿¡æ¯:")
    for i, doc in enumerate(documents):
        print(f"  æ–‡æ¡£ {i+1}: {len(doc.text)} å­—ç¬¦")
        print(f"  å‰200å­—ç¬¦: {doc.text[:200]}...")
    
    # æµ‹è¯•ä¸åŒçš„è¯­ä¹‰æ‹†åˆ†é…ç½®
    configs = [
        {"name": "ä¿å®ˆé…ç½®", "threshold": 95, "buffer": 1},
        {"name": "å½“å‰é…ç½®", "threshold": 90, "buffer": 2}, 
        {"name": "é€‚ä¸­é…ç½®", "threshold": 80, "buffer": 1},
        {"name": "æ•æ„Ÿé…ç½®", "threshold": 70, "buffer": 1},
        {"name": "ææ•æ„Ÿé…ç½®", "threshold": 60, "buffer": 1}
    ]
    
    print(f"\nğŸ” è¯­ä¹‰æ‹†åˆ†æµ‹è¯•:")
    for config in configs:
        print(f"\n--- {config['name']} (é˜ˆå€¼: {config['threshold']}%, ç¼“å†²: {config['buffer']}) ---")
        
        # ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»ºè¯­ä¹‰æ‹†åˆ†å™¨ï¼Œä½†æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è®¾ç½®å‚æ•°
        from app.model.embedding_model import get_embedding_model
        splitter = SemanticSplitterNodeParser(
            breakpoint_percentile_threshold=config['threshold'],
            buffer_size=config['buffer'],
            embed_model=get_embedding_model()
        )
        
        nodes = splitter.get_nodes_from_documents(documents)
        print(f"  ğŸ“Š èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
        
        for j, node in enumerate(nodes):
            # è·å–èŠ‚ç‚¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
            preview = node.text.replace('\n', ' ').strip()[:80]
            print(f"    èŠ‚ç‚¹ {j+1}: {len(node.text)} å­—ç¬¦ - \"{preview}...\"")
    
    # å¯¹æ¯”å¥å­æ‹†åˆ†æ•ˆæœ
    print(f"\nğŸ“ å¥å­æ‹†åˆ†å¯¹æ¯”:")
    sentence_configs = [
        {"name": "å°å—æ‹†åˆ†", "chunk_size": 200, "overlap": 50},
        {"name": "ä¸­å—æ‹†åˆ†", "chunk_size": 400, "overlap": 80},
        {"name": "å¤§å—æ‹†åˆ†", "chunk_size": 800, "overlap": 100}
    ]
    
    for config in sentence_configs:
        print(f"\n--- {config['name']} (å—å¤§å°: {config['chunk_size']}, é‡å : {config['overlap']}) ---")
        splitter = SentenceSplitter(
            chunk_size=config['chunk_size'], 
            chunk_overlap=config['overlap']
        )
        nodes = splitter.get_nodes_from_documents(documents)
        print(f"  ğŸ“Š èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
        
        for j, node in enumerate(nodes):
            preview = node.text.replace('\n', ' ').strip()[:80]
            print(f"    èŠ‚ç‚¹ {j+1}: {len(node.text)} å­—ç¬¦ - \"{preview}...\"")
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ æ€»ç»“:")
    print(f"  - è¯­ä¹‰æ‹†åˆ†ä¼šæ ¹æ®å†…å®¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦è‡ªåŠ¨å†³å®šæ‹†åˆ†ç‚¹")
    print(f"  - é˜ˆå€¼è¶Šä½ï¼Œæ‹†åˆ†è¶Šç»†ç²’åº¦") 
    print(f"  - å¥å­æ‹†åˆ†æŒ‰å›ºå®šå¤§å°æ‹†åˆ†ï¼Œä¸è€ƒè™‘è¯­ä¹‰è¿è´¯æ€§")
    print(f"  - å»ºè®®æ ¹æ®å®é™…åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ‹†åˆ†ç­–ç•¥") 